{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mCkoFdFadPcG",
    "outputId": "56f60b7e-3818-46af-e847-23ad72e63189"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "!pip install keras-tuner\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Bidirectional, LSTM, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import SparseCategoricalCrossentropy\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from kerastuner import HyperModel, BayesianOptimization\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 451
    },
    "id": "bqj4dlcEddgr",
    "outputId": "71509ecd-0910-486b-f628-ed77efc2ea81"
   },
   "outputs": [],
   "source": [
    "tweets = pd.read_csv(\"/content/twitterNews.csv\")\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xiEyUWQrdiQy"
   },
   "outputs": [],
   "source": [
    "features = tweets.iloc[:, 10].values\n",
    "labels = tweets.iloc[:, 1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4jzsIIEPdkj3"
   },
   "outputs": [],
   "source": [
    "def preprocess_text(features):\n",
    "    processed_features_list = []\n",
    "\n",
    "    for sentence in features:\n",
    "        # Remove HTML tags, URLs, and mentions starting with '@'\n",
    "        processed_feature = re.sub(r'<[^>]+>|http\\S+|@\\w+', '', str(sentence))\n",
    "\n",
    "        # Remove all special characters, punctuation, and numbers\n",
    "        processed_feature = re.sub(r'\\W|\\d+', ' ', processed_feature)\n",
    "\n",
    "        # Convert to lowercase\n",
    "        processed_feature = processed_feature.lower()\n",
    "\n",
    "        # Remove single characters\n",
    "        processed_feature = re.sub(r'\\s+[a-zA-Z]\\s+|\\^[a-zA-Z]\\s+', ' ', processed_feature)\n",
    "\n",
    "        # Substituting multiple spaces with single space\n",
    "        processed_feature = re.sub(r'\\s+', ' ', processed_feature).strip()\n",
    "\n",
    "        # Removing prefixed 'b'\n",
    "        processed_feature = re.sub(r'^b\\s+', '', processed_feature)\n",
    "\n",
    "        # Tokenize the text\n",
    "        words = word_tokenize(processed_feature)\n",
    "\n",
    "\n",
    "        # Remove stopwords and lemmatize\n",
    "        processed_text = ' '.join(lemmatizer.lemmatize(word) for word in words if word not in stop_words)\n",
    "\n",
    "        processed_features_list.append(processed_text)\n",
    "\n",
    "    return processed_features_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sj1tnc-8dlhL"
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "processed_features = preprocess_text(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NFFJzYFVdnlp"
   },
   "outputs": [],
   "source": [
    "#tokenization and padding\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(processed_features)\n",
    "word_index = tokenizer.word_index\n",
    "sequences = tokenizer.texts_to_sequences(processed_features)\n",
    "padded_sequences = pad_sequences(sequences,padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W1XNap0wdqTq"
   },
   "outputs": [],
   "source": [
    "# Initialize LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit and transform the textual labels to numerical values\n",
    "encoded_labels = label_encoder.fit_transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3hoI8zo-dsJG"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, encoded_labels, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1kHGIXEFdufv",
    "outputId": "d37081af-d3b7-4991-c14c-d1427620edda"
   },
   "outputs": [],
   "source": [
    "#calculate class weights for imbalanced dataset\n",
    "class_counts = np.bincount(y_train)\n",
    "print(\"class_counts : \",class_counts)\n",
    "total_samples = sum(class_counts)\n",
    "print(\"total_samples : \",total_samples)\n",
    "class_weights = {cls: total_samples / count for cls, count in enumerate(class_counts) }\n",
    "print(\"class_weights : \",class_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R378wgZgdwc-"
   },
   "outputs": [],
   "source": [
    "max_len = len(padded_sequences[0])\n",
    "max_words = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lXdMDPiYdyfX"
   },
   "outputs": [],
   "source": [
    "\n",
    "class SentimentHyperModel(HyperModel):\n",
    "    def build(self, hp):\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(input_dim=max_words, output_dim=hp.Int('embedding_dim', min_value=64, max_value=256, step=32), input_length=max_len))\n",
    "        model.add(Bidirectional(LSTM(units=hp.Int('lstm_units', min_value=64, max_value=128, step=32), return_sequences=True)))\n",
    "        model.add(GlobalMaxPooling1D())  # Remove this layer for now\n",
    "        model.add(Dense(units=hp.Int('dense_units', min_value=64, max_value=256, step=32), activation='relu'))\n",
    "        model.add(Dropout(rate=hp.Float('dense_dropout', min_value=0.2, max_value=0.6, step=0.1)))\n",
    "        model.add(Dense(units=3, activation='softmax'))\n",
    "        optimizer = Adam(learning_rate=hp.Float('learning_rate', min_value=1e-4, max_value=1, sampling='log'))\n",
    "        loss = SparseCategoricalCrossentropy()\n",
    "        model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "onsbDLRDd03F"
   },
   "outputs": [],
   "source": [
    "hypermodel = SentimentHyperModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NcqEoXled6Xk"
   },
   "outputs": [],
   "source": [
    "# Define batch size\n",
    "batch_size = 32\n",
    "\n",
    "# Perform hyperparameter search with BayesianOptimization tuner\n",
    "tuner_bayesian = BayesianOptimization(\n",
    "    hypermodel,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,\n",
    "    directory='./bayesian_batch2',\n",
    "    project_name='sentiment_analysis_bayesian'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-5_zVFZsd9wS",
    "outputId": "5e816bd3-36c3-4e8a-b058-5ea1f15d3a19"
   },
   "outputs": [],
   "source": [
    "# Define the callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(patience=10, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(factor=0.2, patience=2)\n",
    "]\n",
    "\n",
    "# Perform hyperparameter search with BayesianOptimization tuner\n",
    "tuner_bayesian.search(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=10,\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    class_weight=class_weights,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CSB6rYX1eI2E",
    "outputId": "6dca3571-382f-43b0-c4a9-d1be8317eedd"
   },
   "outputs": [],
   "source": [
    "best_model = tuner_bayesian.get_best_models(num_models=1)[0]\n",
    "best_model.save(\"model.h5\")\n",
    "# Evaluate the best model from BayesianOptimization tuner\n",
    "test_loss_bayesian, test_accuracy_bayesian = best_model.evaluate(X_test, y_test)\n",
    "print(\"\\nBayesian Optimization:\")\n",
    "print(\"Test Loss:\", test_loss_bayesian)\n",
    "print(\"Test Accuracy:\", test_accuracy_bayesian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 582
    },
    "id": "ldwreVrkegMS",
    "outputId": "febd7da3-d63a-42bf-8fc5-1955fab4b86b"
   },
   "outputs": [],
   "source": [
    "#confussion matrix\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have trained your model and obtained predictions\n",
    "# Let's say your model predictions are stored in y_pred\n",
    "# Generate predictions for the test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Select the class with the highest probability as the predicted class\n",
    "y_pred_classes = np.argmax(y_pred, axis=-1)\n",
    "\n",
    "# Now y_pred_classes contains the predicted class labels\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_classes)\n",
    "\n",
    "# Display confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
